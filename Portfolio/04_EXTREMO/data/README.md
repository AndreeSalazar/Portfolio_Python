# üìä Datasets - Nivel EXTREMO

Esta carpeta contiene los datasets descargados para proyectos de nivel EXTREMO.

## üì¶ Datasets Incluidos

### 1. Store Sales Time Series Forecasting (COMPLETO)
**Fuente**: Kaggle  
**ID**: `competitions/store-sales-time-series-forecasting`  
**Carpeta**: `store_sales_completo/`

**Descripci√≥n**:
- Dataset COMPLETO de series temporales
- Big Data - M√∫ltiples millones de registros
- Perfecto para an√°lisis predictivo avanzado
- Requiere optimizaci√≥n extrema

**Estructura**:
- `train.csv` - Datos completos de entrenamiento (millones de registros)
- `test.csv` - Datos de prueba
- `stores.csv` - Informaci√≥n de tiendas
- `oil.csv` - Precios de petr√≥leo hist√≥ricos
- `holidays_events.csv` - Calendario completo de festivos
- `transactions.csv` - Transacciones detalladas

**Uso en proyectos**:
- Forecasting avanzado de ventas
- An√°lisis de estacionalidad compleja
- Modelos de Machine Learning avanzados
- Optimizaci√≥n de inventario a gran escala
- An√°lisis de factores externos m√∫ltiples

**Stack a demostrar**:
- PostgreSQL: Particionado, √≠ndices avanzados, paralelizaci√≥n
- Python: Procesamiento eficiente, optimizaci√≥n de memoria
- Jupyter: Pipeline ML completo documentado
- Excel: Reportes ejecutivos automatizados

---

### 2. Brazilian E-commerce (COMPLETO)
**Fuente**: Kaggle  
**ID**: `olistbr/brazilian-ecommerce`  
**Carpeta**: `brazilian_ecommerce_completo/`

**Descripci√≥n**:
- Dataset COMPLETO de e-commerce brasile√±o
- Todas las tablas relacionadas
- M√∫ltiples millones de registros
- Excelente para proyectos end-to-end

**Estructura completa**:
- `olist_customers_dataset.csv` - Base completa de clientes
- `olist_orders_dataset.csv` - Todas las √≥rdenes
- `olist_order_items_dataset.csv` - Todos los items (1M+)
- `olist_products_dataset.csv` - Cat√°logo completo
- `olist_sellers_dataset.csv` - Todos los vendedores
- `olist_geolocation_dataset.csv` - Datos geogr√°ficos completos
- `olist_order_reviews_dataset.csv` - Reviews completas
- `olist_order_payments_dataset.csv` - Pagos completos

**Uso en proyectos**:
- Sistema completo de an√°lisis de e-commerce
- An√°lisis de cohortes a gran escala
- Optimizaci√≥n de log√≠stica
- An√°lisis predictivo completo
- Detecci√≥n de anomal√≠as avanzada

---

### 3. YouTube Trending Dataset
**Fuente**: Kaggle  
**ID**: `datasnaek/youtube-new`  
**Carpeta**: `youtube_trending/`

**Descripci√≥n**:
- Dataset grande de videos de YouTube
- Datos de trending por pa√≠s
- M√∫ltiples a√±os de datos
- Perfecto para an√°lisis de Big Data

**Estructura esperada**:
- Archivos CSV por pa√≠s/regi√≥n
- Columnas: video_id, title, channel_title, category_id, views, likes, dislikes, comment_count, trending_date, etc.

**Uso en proyectos**:
- An√°lisis de tendencias de contenido
- Predicci√≥n de viralidad
- An√°lisis de engagement
- Optimizaci√≥n de contenido
- An√°lisis de comportamiento de usuarios

---

## üöÄ C√≥mo Usar Estos Datasets

### Paso 1: Procesamiento en Chunks
```python
# scripts/procesamiento_big_data.py
import pandas as pd
from sqlalchemy import create_engine
import numpy as np

# Procesar en chunks peque√±os
chunk_size = 5000
engine = create_engine('postgresql://postgres:password@localhost:5432/big_data_analysis')

# Procesar archivo grande
for i, chunk in enumerate(pd.read_csv('data/store_sales_completo/train.csv', chunksize=chunk_size)):
    print(f"Procesando chunk {i+1}...")
    # Transformaciones
    chunk_processed = transform_chunk(chunk)
    # Cargar a PostgreSQL
    chunk_processed.to_sql('ventas', engine, if_exists='append', index=False, method='multi')
```

### Paso 2: Optimizaci√≥n Extrema de PostgreSQL
```sql
-- Particionado de tablas por fecha
CREATE TABLE ventas (
    id SERIAL,
    fecha DATE NOT NULL,
    -- otras columnas
) PARTITION BY RANGE (fecha);

-- Crear particiones
CREATE TABLE ventas_2020 PARTITION OF ventas
    FOR VALUES FROM ('2020-01-01') TO ('2021-01-01');

CREATE TABLE ventas_2021 PARTITION OF ventas
    FOR VALUES FROM ('2021-01-01') TO ('2022-01-01');

-- √çndices avanzados
CREATE INDEX CONCURRENTLY idx_ventas_fecha_store ON ventas(fecha, store_id);
CREATE INDEX CONCURRENTLY idx_ventas_producto ON ventas(product_id) WHERE cantidad > 0;

-- Vistas materializadas con refresh autom√°tico
CREATE MATERIALIZED VIEW ventas_agregadas AS
SELECT 
    DATE_TRUNC('month', fecha) AS mes,
    store_id,
    SUM(total) AS ingresos,
    COUNT(*) AS transacciones
FROM ventas
GROUP BY mes, store_id;

CREATE UNIQUE INDEX ON ventas_agregadas(mes, store_id);
```

### Paso 3: Pipeline ML Completo
```python
# notebooks/pipeline_ml_extremo.ipynb
# Feature engineering avanzado
# Validaci√≥n cruzada
# Optimizaci√≥n de hiperpar√°metros
# Deployment b√°sico
```

---

## üìù Notas Importantes

- ‚ö†Ô∏è Estos datasets son EXTREMADAMENTE grandes
- ‚úÖ SIEMPRE usa procesamiento en chunks
- ‚úÖ Optimiza PostgreSQL con particionado
- ‚úÖ Usa √≠ndices CONCURRENTLY para no bloquear
- ‚úÖ Considera usar muestreo para an√°lisis exploratorio
- ‚úÖ Mide y documenta tiempos de ejecuci√≥n
- ‚úÖ Considera usar paralelizaci√≥n (multiprocessing)
- ‚úÖ Monitorea el uso de memoria

---

## üîó Enlaces √ötiles

- **Kaggle**: https://www.kaggle.com/datasets
- **Documentaci√≥n del Portfolio**: ../FUENTES_DATOS_Y_PROYECTOS.md
- **Gu√≠a de Big Data**: Ver secci√≥n EXTREMO en FUENTES_DATOS_Y_PROYECTOS.md

---

**√öltima actualizaci√≥n**: Diciembre 2024

**Nota**: Estos proyectos est√°n dise√±ados para demostrar expertise y capacidad de trabajar con Big Data.

