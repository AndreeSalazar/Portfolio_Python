# üìä Datos EXTREMOS - Nivel SENIOR/EXPERTO/√âLITE

## üéØ Datasets de Big Data Generados

Estos datasets est√°n dise√±ados para demostrar habilidades de nivel **SENIOR/EXPERTO/√âLITE** en Data Analysis.

---

## üì¶ Dataset 1: Store Sales Time Series Forecasting (COMPLETO)

### Caracter√≠sticas EXTREMAS:
- ‚úÖ **2+ MILLONES de registros** de ventas
- ‚úÖ **200 tiendas** en m√∫ltiples estados
- ‚úÖ **10,000 productos** diferentes
- ‚úÖ **5 a√±os de datos** (2013-2017)
- ‚úÖ **Variables externas**: Precios de petr√≥leo hist√≥ricos
- ‚úÖ **Calendario completo**: Festivos y eventos
- ‚úÖ **Transacciones diarias** por tienda

### Estructura:
```
store_sales_completo/
‚îú‚îÄ‚îÄ train.csv              # 2+ millones de registros
‚îú‚îÄ‚îÄ stores.csv             # 200 tiendas
‚îú‚îÄ‚îÄ products.csv           # 10,000 productos
‚îú‚îÄ‚îÄ oil.csv                # Precios hist√≥ricos de petr√≥leo (2010-2024)
‚îú‚îÄ‚îÄ holidays_events.csv    # Calendario completo de eventos
‚îî‚îÄ‚îÄ transactions.csv       # Transacciones diarias
```

### Desaf√≠os T√©cnicos:
- üî• **Procesamiento en chunks** obligatorio
- üî• **Particionado de tablas** en PostgreSQL
- üî• **√çndices avanzados** necesarios
- üî• **Optimizaci√≥n de memoria** cr√≠tica
- üî• **An√°lisis de estacionalidad** compleja
- üî• **Forecasting avanzado** con m√∫ltiples variables

### Proyectos Posibles:
1. **Forecasting de Ventas Multi-Tienda**
   - Modelos de series temporales avanzados
   - ARIMA, Prophet, LSTM
   - Optimizaci√≥n de inventario

2. **An√°lisis de Factores Externos**
   - Impacto del precio del petr√≥leo
   - Efecto de festivos y eventos
   - An√°lisis de correlaciones complejas

3. **Optimizaci√≥n de Operaciones**
   - An√°lisis de performance por tienda
   - Identificaci√≥n de patrones an√≥malos
   - Recomendaciones de optimizaci√≥n

---

## üì¶ Dataset 2: Brazilian E-commerce (COMPLETO)

### Caracter√≠sticas EXTREMAS:
- ‚úÖ **100,000 clientes** √∫nicos
- ‚úÖ **200,000 √≥rdenes** completas
- ‚úÖ **500,000+ items** de √≥rdenes
- ‚úÖ **50,000 productos** en cat√°logo
- ‚úÖ **10,000 vendedores** activos
- ‚úÖ **300,000 reviews** de clientes
- ‚úÖ **600,000 pagos** procesados
- ‚úÖ **Datos geogr√°ficos** completos

### Estructura:
```
brazilian_ecommerce_completo/
‚îú‚îÄ‚îÄ customers.csv          # 100K clientes
‚îú‚îÄ‚îÄ sellers.csv            # 10K vendedores
‚îú‚îÄ‚îÄ products.csv           # 50K productos
‚îú‚îÄ‚îÄ orders.csv             # 200K √≥rdenes
‚îú‚îÄ‚îÄ order_items.csv        # 500K+ items
‚îú‚îÄ‚îÄ order_reviews.csv      # 300K reviews
‚îú‚îÄ‚îÄ order_payments.csv     # 600K pagos
‚îî‚îÄ‚îÄ geolocation.csv        # Datos geogr√°ficos
```

### Desaf√≠os T√©cnicos:
- üî• **JOINs complejos** entre 8 tablas
- üî• **An√°lisis de cohortes** a gran escala
- üî• **Customer Lifetime Value** avanzado
- üî• **Detecci√≥n de fraude** en pagos
- üî• **An√°lisis de sentimiento** en reviews
- üî• **Optimizaci√≥n de log√≠stica** geogr√°fica

### Proyectos Posibles:
1. **Sistema de Recomendaci√≥n Completo**
   - Collaborative Filtering
   - Content-Based Filtering
   - Hybrid Recommendations

2. **An√°lisis de Churn Predictivo**
   - Identificar clientes en riesgo
   - Modelos de clasificaci√≥n avanzados
   - Estrategias de retenci√≥n

3. **Optimizaci√≥n de Log√≠stica**
   - An√°lisis geogr√°fico avanzado
   - Optimizaci√≥n de rutas
   - Predicci√≥n de tiempos de entrega

4. **Detecci√≥n de Anomal√≠as**
   - Fraude en pagos
   - Vendedores sospechosos
   - Productos an√≥malos

---

## üì¶ Dataset 3: YouTube Trending (Multi-Pa√≠s)

### Caracter√≠sticas EXTREMAS:
- ‚úÖ **13 pa√≠ses** diferentes
- ‚úÖ **100,000+ videos** trending por pa√≠s
- ‚úÖ **7 meses de datos** (Nov 2017 - Jun 2018)
- ‚úÖ **14 categor√≠as** de contenido
- ‚úÖ **M√©tricas completas**: Views, Likes, Comments
- ‚úÖ **Datos temporales** granulares

### Estructura:
```
youtube_trending/
‚îú‚îÄ‚îÄ youtube_trending_US.csv
‚îú‚îÄ‚îÄ youtube_trending_GB.csv
‚îú‚îÄ‚îÄ youtube_trending_CA.csv
‚îú‚îÄ‚îÄ youtube_trending_AU.csv
‚îú‚îÄ‚îÄ youtube_trending_DE.csv
‚îú‚îÄ‚îÄ youtube_trending_FR.csv
‚îú‚îÄ‚îÄ youtube_trending_ES.csv
‚îú‚îÄ‚îÄ youtube_trending_IT.csv
‚îú‚îÄ‚îÄ youtube_trending_BR.csv
‚îú‚îÄ‚îÄ youtube_trending_MX.csv
‚îú‚îÄ‚îÄ youtube_trending_IN.csv
‚îú‚îÄ‚îÄ youtube_trending_JP.csv
‚îî‚îÄ‚îÄ youtube_trending_KR.csv
```

### Desaf√≠os T√©cnicos:
- üî• **Integraci√≥n de m√∫ltiples fuentes**
- üî• **An√°lisis comparativo** entre pa√≠ses
- üî• **Predicci√≥n de viralidad**
- üî• **An√°lisis de engagement** avanzado
- üî• **NLP** en t√≠tulos y descripciones
- üî• **Clustering** de contenido

### Proyectos Posibles:
1. **Predicci√≥n de Viralidad**
   - Modelos de regresi√≥n avanzados
   - Feature engineering complejo
   - An√°lisis de factores de √©xito

2. **An√°lisis de Tendencias Globales**
   - Comparaci√≥n entre pa√≠ses
   - Identificaci√≥n de patrones culturales
   - An√°lisis de preferencias regionales

3. **Sistema de Recomendaci√≥n de Contenido**
   - Basado en comportamiento
   - Optimizaci√≥n de engagement
   - Personalizaci√≥n avanzada

---

## üöÄ C√≥mo Trabajar con Estos Datos

### 1. Procesamiento en Chunks (OBLIGATORIO)

```python
import pandas as pd
from sqlalchemy import create_engine

chunk_size = 10000
engine = create_engine('postgresql://postgres:password@localhost:5432/big_data')

for i, chunk in enumerate(pd.read_csv('data/store_sales_completo/train.csv', chunksize=chunk_size)):
    print(f"Procesando chunk {i+1}...")
    # Transformaciones
    chunk_processed = transform_chunk(chunk)
    # Cargar a PostgreSQL
    chunk_processed.to_sql('ventas', engine, if_exists='append', index=False, method='multi')
```

### 2. Optimizaci√≥n Extrema de PostgreSQL

```sql
-- Particionado por fecha
CREATE TABLE ventas (
    date DATE NOT NULL,
    store_nbr INTEGER,
    item_nbr INTEGER,
    unit_sales INTEGER
) PARTITION BY RANGE (date);

-- Crear particiones anuales
CREATE TABLE ventas_2013 PARTITION OF ventas
    FOR VALUES FROM ('2013-01-01') TO ('2014-01-01');
CREATE TABLE ventas_2014 PARTITION OF ventas
    FOR VALUES FROM ('2014-01-01') TO ('2015-01-01');
-- ... m√°s particiones

-- √çndices avanzados
CREATE INDEX CONCURRENTLY idx_ventas_store_date 
    ON ventas(store_nbr, date);
CREATE INDEX CONCURRENTLY idx_ventas_item_date 
    ON ventas(item_nbr, date) WHERE unit_sales > 0;

-- Vistas materializadas
CREATE MATERIALIZED VIEW ventas_mensuales AS
SELECT 
    DATE_TRUNC('month', date) AS mes,
    store_nbr,
    SUM(unit_sales) AS total_ventas,
    COUNT(*) AS num_transacciones
FROM ventas
GROUP BY mes, store_nbr;

CREATE UNIQUE INDEX ON ventas_mensuales(mes, store_nbr);
```

### 3. Feature Engineering Avanzado

```python
# Crear features temporales
df['year'] = pd.to_datetime(df['date']).dt.year
df['month'] = pd.to_datetime(df['date']).dt.month
df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6])
df['is_month_start'] = pd.to_datetime(df['date']).dt.is_month_start
df['is_month_end'] = pd.to_datetime(df['date']).dt.is_month_end

# Features de lag
df['sales_lag_7'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(7)
df['sales_lag_30'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(30)

# Features de rolling
df['sales_rolling_7'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].rolling(7).mean()
df['sales_rolling_30'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].rolling(30).mean()
```

### 4. Machine Learning Avanzado

```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import TimeSeriesSplit
import xgboost as xgb

# Time Series Cross-Validation
tscv = TimeSeriesSplit(n_splits=5)

# Modelos avanzados
models = {
    'RandomForest': RandomForestRegressor(n_estimators=100, n_jobs=-1),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, n_jobs=-1)
}

# Entrenar y validar
for name, model in models.items():
    scores = []
    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        model.fit(X_train, y_train)
        score = model.score(X_val, y_val)
        scores.append(score)
    
    print(f"{name}: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})")
```

---

## üìä Estad√≠sticas de los Datasets

| Dataset | Registros | Tama√±o Aprox | Tablas | Complejidad |
|---------|-----------|--------------|--------|-------------|
| Store Sales | 2+ millones | ~500 MB | 6 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Brazilian E-commerce | 1.2+ millones | ~300 MB | 8 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| YouTube Trending | 1.3+ millones | ~200 MB | 13 archivos | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Total**: ~4.5 millones de registros, ~1 GB de datos

---

## ‚ö†Ô∏è Requisitos T√©cnicos

### Hardware Recomendado:
- **RAM**: M√≠nimo 16 GB (32 GB recomendado)
- **Disco**: 5+ GB libres
- **CPU**: Multi-core recomendado

### Software:
- **PostgreSQL 12+** con particionado habilitado
- **Python 3.8+** con pandas, numpy, scikit-learn
- **Jupyter** para an√°lisis interactivo
- **Memoria suficiente** para procesamiento

### T√©cnicas Necesarias:
- ‚úÖ Procesamiento en chunks
- ‚úÖ Optimizaci√≥n de queries SQL
- ‚úÖ Particionado de tablas
- ‚úÖ √çndices avanzados
- ‚úÖ Feature engineering
- ‚úÖ Machine Learning avanzado
- ‚úÖ Validaci√≥n cruzada temporal
- ‚úÖ Optimizaci√≥n de hiperpar√°metros

---

## üéØ Proyectos Recomendados para Demostrar Expertise

### Proyecto 1: Sistema de Forecasting Completo
- Forecasting multi-tienda y multi-producto
- Modelos de ML avanzados
- Optimizaci√≥n de hiperpar√°metros
- Dashboard interactivo

### Proyecto 2: An√°lisis de E-commerce End-to-End
- An√°lisis de cohortes avanzado
- Sistema de recomendaci√≥n
- Detecci√≥n de fraude
- Optimizaci√≥n de log√≠stica

### Proyecto 3: An√°lisis de Contenido Viral
- Predicci√≥n de viralidad
- An√°lisis de tendencias globales
- NLP en t√≠tulos/descripciones
- Clustering de contenido

---

## üìù Notas Finales

Estos datasets est√°n dise√±ados para:
- ‚úÖ Demostrar capacidad de trabajar con **Big Data**
- ‚úÖ Mostrar habilidades de **optimizaci√≥n avanzada**
- ‚úÖ Aplicar t√©cnicas de **Machine Learning complejas**
- ‚úÖ Crear soluciones **end-to-end** profesionales
- ‚úÖ Demostrar expertise en **an√°lisis predictivo**

**¬°Estos proyectos impresionar√°n a cualquier reclutador!** üöÄ

---

**√öltima actualizaci√≥n**: Diciembre 2024

