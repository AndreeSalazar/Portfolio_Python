# ğŸ“¥ Resumen Completo de Descargas - Portfolio Data Analyst

Este documento resume TODOS los datasets descargados para el portfolio completo, organizados por nivel.

---

## ğŸ“Š Resumen General

| Nivel | Datasets | TamaÃ±o Aproximado | Complejidad |
|-------|----------|-------------------|-------------|
| ğŸ“˜ **BÃ¡sico** | 3 datasets | ~100-500 MB | Baja |
| ğŸ“— **Intermedio** | 3 datasets | ~500 MB - 2 GB | Media |
| ğŸ“™ **Avanzado** | 3 datasets | ~2-5 GB | Alta |
| ğŸ“• **EXTREMO** | 3 datasets | ~5-20 GB | Muy Alta |
| **TOTAL** | **12 datasets** | **~10-30 GB** | - |

---

## ğŸ“˜ NIVEL BÃSICO

### Dataset 1: Retail Sales Dataset
- **Fuente**: Kaggle
- **ID**: `rohitsahoo/sales-forecasting`
- **UbicaciÃ³n**: `01_Basico/data/retail_sales/`
- **TamaÃ±o**: ~50-100 MB
- **Registros**: ~50K-100K
- **Uso**: AnÃ¡lisis bÃ¡sico de ventas, JOINs simples, visualizaciones bÃ¡sicas

### Dataset 2: Superstore Sales Dataset
- **Fuente**: Kaggle
- **ID**: `vivek468/superstore-dataset-final`
- **UbicaciÃ³n**: `01_Basico/data/superstore/`
- **TamaÃ±o**: ~50-100 MB
- **Registros**: ~10K-50K
- **Uso**: Dashboards, anÃ¡lisis de profitabilidad, tablas dinÃ¡micas

### Dataset 3: HR Analytics Dataset
- **Fuente**: Kaggle
- **ID**: `arindam235/startup-investments-crunchbase`
- **UbicaciÃ³n**: `01_Basico/data/hr_analytics/`
- **TamaÃ±o**: ~20-50 MB
- **Registros**: ~15K
- **Uso**: AnÃ¡lisis de empleados, performance, rotaciÃ³n

**Total Nivel BÃ¡sico**: ~120-250 MB

---

## ğŸ“— NIVEL INTERMEDIO

### Dataset 1: E-commerce Customer Data
- **Fuente**: Kaggle
- **ID**: `carrie1/ecommerce-data`
- **UbicaciÃ³n**: `02_Intermedio/data/ecommerce/`
- **TamaÃ±o**: ~200-500 MB
- **Registros**: ~500K
- **Uso**: AnÃ¡lisis de cohortes, CLV, JOINs complejos, Window Functions

### Dataset 2: Online Retail Dataset (UCI)
- **Fuente**: UCI Machine Learning Repository
- **URL**: https://archive.ics.uci.edu/ml/datasets/Online+Retail
- **UbicaciÃ³n**: `02_Intermedio/data/online_retail/`
- **TamaÃ±o**: ~50-100 MB
- **Registros**: ~540K
- **Uso**: RFM Analysis, anÃ¡lisis temporal, detecciÃ³n de anomalÃ­as

### Dataset 3: Marketing Analytics Dataset
- **Fuente**: Kaggle
- **ID**: `datasnaek/marketing-analytics`
- **UbicaciÃ³n**: `02_Intermedio/data/marketing/`
- **TamaÃ±o**: ~100-200 MB
- **Registros**: ~100K-200K
- **Uso**: AnÃ¡lisis de campaÃ±as, ROI, segmentaciÃ³n, optimizaciÃ³n

**Total Nivel Intermedio**: ~350-800 MB

---

## ğŸ“™ NIVEL AVANZADO

### Dataset 1: Brazilian E-commerce Dataset
- **Fuente**: Kaggle
- **ID**: `olistbr/brazilian-ecommerce`
- **UbicaciÃ³n**: `03_Avanzado/data/brazilian_ecommerce/`
- **TamaÃ±o**: ~500 MB - 2 GB
- **Registros**: ~100K Ã³rdenes, 1M+ items
- **Uso**: AnÃ¡lisis completo, optimizaciÃ³n avanzada, ETL robusto, anÃ¡lisis predictivo

### Dataset 2: Store Sales Time Series Forecasting
- **Fuente**: Kaggle
- **ID**: `competitions/store-sales-time-series-forecasting`
- **UbicaciÃ³n**: `03_Avanzado/data/store_sales/`
- **TamaÃ±o**: ~1-3 GB
- **Registros**: ~1M+
- **Uso**: Forecasting, anÃ¡lisis de estacionalidad, modelos predictivos, optimizaciÃ³n de inventario

### Dataset 3: Banking Dataset
- **Fuente**: Kaggle
- **ID**: `sriharipramod/bank-customer-data`
- **UbicaciÃ³n**: `03_Avanzado/data/banking/`
- **TamaÃ±o**: ~200-500 MB
- **Registros**: ~100K-500K
- **Uso**: AnÃ¡lisis de riesgo, detecciÃ³n de fraude, segmentaciÃ³n, churn analysis

**Total Nivel Avanzado**: ~1.7-5.5 GB

---

## ğŸ“• NIVEL EXTREMO

### Dataset 1: Store Sales Time Series Forecasting (COMPLETO)
- **Fuente**: Kaggle
- **ID**: `competitions/store-sales-time-series-forecasting`
- **UbicaciÃ³n**: `04_EXTREMO/data/store_sales_completo/`
- **TamaÃ±o**: ~5-10 GB
- **Registros**: MÃºltiples millones
- **Uso**: Big Data analysis, forecasting avanzado, optimizaciÃ³n extrema, particionado

### Dataset 2: Brazilian E-commerce (COMPLETO)
- **Fuente**: Kaggle
- **ID**: `olistbr/brazilian-ecommerce`
- **UbicaciÃ³n**: `04_EXTREMO/data/brazilian_ecommerce_completo/`
- **TamaÃ±o**: ~2-5 GB
- **Registros**: MÃºltiples millones
- **Uso**: Proyectos end-to-end, anÃ¡lisis a gran escala, optimizaciÃ³n avanzada

### Dataset 3: YouTube Trending Dataset
- **Fuente**: Kaggle
- **ID**: `datasnaek/youtube-new`
- **UbicaciÃ³n**: `04_EXTREMO/data/youtube_trending/`
- **TamaÃ±o**: ~3-5 GB
- **Registros**: MÃºltiples millones
- **Uso**: AnÃ¡lisis de Big Data, predicciÃ³n de viralidad, anÃ¡lisis de comportamiento

**Total Nivel EXTREMO**: ~10-20 GB

---

## ğŸš€ CÃ³mo Descargar

### OpciÃ³n 1: Descargar Todos (Recomendado)
```bash
python Portfolio/scripts/descargar_todos_datasets.py
```

### OpciÃ³n 2: Descargar por Nivel
```bash
# Nivel BÃ¡sico
python Portfolio/scripts/descargar_basico.py

# Nivel Intermedio
python Portfolio/scripts/descargar_intermedio.py

# Nivel Avanzado
python Portfolio/scripts/descargar_avanzado.py

# Nivel EXTREMO
python Portfolio/scripts/descargar_extremo.py
```

### OpciÃ³n 3: Descarga Manual
1. Revisa los README.md en cada carpeta `data/`
2. Descarga manualmente desde Kaggle/UCI
3. Coloca los archivos en las carpetas correspondientes

---

## ğŸ“‹ Requisitos Previos

### Software Necesario
- âœ… Python 3.7+
- âœ… pip install kaggle pandas requests
- âœ… Cuenta de Kaggle (gratuita)
- âœ… Archivo `kaggle.json` configurado

### ConfiguraciÃ³n de Kaggle
1. Crear cuenta en https://www.kaggle.com/
2. Ir a Account â†’ API â†’ Create New API Token
3. Descargar `kaggle.json`
4. Colocar en:
   - Windows: `C:\Users\tu-usuario\.kaggle\kaggle.json`
   - Linux/Mac: `~/.kaggle/kaggle.json`
5. Permisos (Linux/Mac): `chmod 600 ~/.kaggle/kaggle.json`

### Espacio en Disco
- **MÃ­nimo recomendado**: 30 GB libres
- **Ideal**: 50+ GB libres
- Los datasets EXTREMOS requieren mÃ¡s espacio

---

## ğŸ“ Estructura de Carpetas

```
Portfolio/
â”œâ”€â”€ 01_Basico/
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ retail_sales/
â”‚       â”œâ”€â”€ superstore/
â”‚       â””â”€â”€ hr_analytics/
â”œâ”€â”€ 02_Intermedio/
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ ecommerce/
â”‚       â”œâ”€â”€ online_retail/
â”‚       â””â”€â”€ marketing/
â”œâ”€â”€ 03_Avanzado/
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ brazilian_ecommerce/
â”‚       â”œâ”€â”€ store_sales/
â”‚       â””â”€â”€ banking/
â””â”€â”€ 04_EXTREMO/
    â””â”€â”€ data/
        â”œâ”€â”€ README.md
        â”œâ”€â”€ store_sales_completo/
        â”œâ”€â”€ brazilian_ecommerce_completo/
        â””â”€â”€ youtube_trending/
```

---

## âœ… Checklist de Descarga

### Antes de Descargar
- [ ] Python instalado y funcionando
- [ ] Kaggle API instalada (`pip install kaggle`)
- [ ] Cuenta de Kaggle creada
- [ ] `kaggle.json` configurado correctamente
- [ ] Espacio en disco suficiente (30+ GB)
- [ ] ConexiÃ³n a internet estable

### Durante la Descarga
- [ ] Ejecutar script de descarga
- [ ] Verificar que las carpetas se crean correctamente
- [ ] Monitorear el progreso de descarga
- [ ] Verificar que no hay errores

### DespuÃ©s de Descargar
- [ ] Verificar que los archivos estÃ¡n en las carpetas correctas
- [ ] Revisar los README.md de cada nivel
- [ ] Probar cargar un dataset pequeÃ±o en Python
- [ ] Verificar que los archivos no estÃ¡n corruptos

---

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "Kaggle API no disponible"
```bash
pip install kaggle
```

### Error: "Authentication failed"
- Verifica que `kaggle.json` estÃ© en la ubicaciÃ³n correcta
- Verifica que el archivo tenga el formato correcto
- Regenera el token desde Kaggle si es necesario

### Error: "Out of disk space"
- Libera espacio en disco
- Descarga solo los niveles necesarios
- Considera usar almacenamiento externo

### Error: "Connection timeout"
- Verifica tu conexiÃ³n a internet
- Intenta descargar de nuevo
- Considera descargar manualmente desde Kaggle

### Datasets muy grandes
- Usa los scripts de procesamiento en chunks
- Considera usar muestreo para anÃ¡lisis exploratorio
- Optimiza PostgreSQL con Ã­ndices y particionado

---

## ğŸ“ Notas Importantes

1. **Tiempo de Descarga**: 
   - BÃ¡sico: 5-15 minutos
   - Intermedio: 15-30 minutos
   - Avanzado: 30-60 minutos
   - EXTREMO: 1-3 horas (dependiendo de conexiÃ³n)

2. **Procesamiento**:
   - Los datasets grandes requieren procesamiento en chunks
   - Usa Ã­ndices en PostgreSQL para optimizar
   - Considera muestreo para anÃ¡lisis exploratorio

3. **Almacenamiento**:
   - Los datasets estÃ¡n en `.gitignore` por defecto
   - No subas datasets grandes a Git
   - Usa Git LFS si necesitas versionar datos

4. **DocumentaciÃ³n**:
   - Cada nivel tiene su README.md con detalles
   - Consulta `FUENTES_DATOS_Y_PROYECTOS.md` para guÃ­as completas
   - Revisa `DATASETS_RECOMENDADOS.md` para mÃ¡s informaciÃ³n

---

## ğŸ”— Enlaces Ãštiles

- **Kaggle**: https://www.kaggle.com/datasets
- **UCI Repository**: https://archive.ics.uci.edu/ml/index.php
- **DocumentaciÃ³n Portfolio**: `FUENTES_DATOS_Y_PROYECTOS.md`
- **GuÃ­a de Datasets**: `DATASETS_RECOMENDADOS.md`
- **Resumen Ejecutivo**: `RESUMEN_EJECUTIVO.md`

---

## ğŸ“Š EstadÃ­sticas Finales

- **Total de Datasets**: 12
- **Total de Niveles**: 4
- **TamaÃ±o Total Estimado**: 10-30 GB
- **Tiempo Total de Descarga**: 1-4 horas (dependiendo de conexiÃ³n)
- **Stack TecnolÃ³gico Cubierto**: PostgreSQL, Python, Jupyter, Excel, Git

---

**Ãšltima actualizaciÃ³n**: Diciembre 2024

**Nota**: Este documento se actualiza cuando se agregan nuevos datasets o se modifican los existentes.

